use core::ptr::null_mut;
use wdk_sys::ntddk::{
    IoAllocateMdl, IoFreeMdl, MmMapLockedPagesSpecifyCache, MmProbeAndLockPages, MmUnlockPages,
    MmUnmapLockedPages,
};
use wdk_sys::{
    MdlMappingNoExecute, MDL, PUCHAR, _LOCK_OPERATION::IoModifyAccess,
    _MEMORY_CACHING_TYPE::MmCached, _MM_PAGE_PRIORITY::HighPagePriority, _MODE::KernelMode,
};
use wdk_sys::ntddk::KeGetCurrentIrql;
use spin::Mutex;

/// Global lock to prevent concurrent memory modifications which can lead to race conditions.
static MDL_LOCK: Mutex<()> = Mutex::new(());

/// Memory Descriptor List (MDL) wrapper for safe kernel memory modification.
pub struct Mdl {
    /// Pointer to the MDL structure.
    mdl: *mut MDL,

    /// Mapped kernel address of the locked memory.
    mapped_address: PUCHAR,

    /// Guard to release the lock when the MDL is dropped.
    _guard: Option<spin::MutexGuard<'static, ()>>,
}

impl Mdl {
    /// Creates a new `Mdl`.
    ///
    /// # Arguments
    ///
    /// * `dest` - Target memory address to be modified.
    /// * `size` - Size of the memory region to lock.
    pub fn new(dest: *const u8, size: usize) -> Option<Self> {
        if dest.is_null() || size == 0 {
            wdk::println!("Invalid Parameters");
            return None;
        }

        unsafe {
            // MmProbeAndLockPages MUST be called at IRQL <= APC_LEVEL
            if KeGetCurrentIrql() > wdk_sys::APC_LEVEL as u8 {
                wdk::println!("IRQL too high for MmProbeAndLockPages: {}", KeGetCurrentIrql());
                return None;
            }

            // Acquire global lock
            let guard = MDL_LOCK.lock();
            // Allocate an MDL
            let mdl = IoAllocateMdl(dest as _, size as u32, 0, 0, null_mut());
            if mdl.is_null() {
                return None;
            }

            // Lock the pages for modification
            MmProbeAndLockPages(mdl, KernelMode as i8, IoModifyAccess);

            // Map the locked pages for kernel access
            let mapped_address = MmMapLockedPagesSpecifyCache(
                mdl,
                KernelMode as i8,
                MmCached,
                null_mut(),
                0,
                HighPagePriority as u32 | MdlMappingNoExecute,
            ) as *mut u8;

            if mapped_address.is_null() {
                wdk::println!("Failed to map blocked pages");
                MmUnlockPages(mdl);
                IoFreeMdl(mdl);
                return None;
            }

            Some(Self {
                mdl,
                mapped_address,
                _guard: Some(guard),
            })
        }
    }

    /// Copies memory to the mapped address.
    ///
    /// # Arguments
    ///
    /// * `src` - Pointer to the source data.
    /// * `size` - Size of the data to copy.
    pub fn copy(&self, src: *const u8, size: usize) {
        if src.is_null() || self.mapped_address.is_null() {
            wdk::println!("Invalid address in the memory copy.");
            return;
        }

        unsafe {
            core::ptr::copy_nonoverlapping(src, self.mapped_address, size);
        }
    }
}

impl Drop for Mdl {
    /// Cleans up the MDL and releases memory when dropped.
    fn drop(&mut self) {
        unsafe {
            if !self.mapped_address.is_null() {
                MmUnmapLockedPages(self.mapped_address as _, self.mdl);
            }

            if !self.mdl.is_null() {
                MmUnlockPages(self.mdl);
                IoFreeMdl(self.mdl);
            }
        }
    }
}
